{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process for scraping used car listings for an analysis of value over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for scraping listings from website #1\n",
    "def web_scrape_1(URL_base_1):\n",
    "    \n",
    "    listing_dic = {} # dictionary for storing scraped listings\n",
    "    entries_completed = 0 # entry counter\n",
    "    page_num = 0 # listings page counter\n",
    "    \n",
    "\n",
    "    while True:\n",
    "        \n",
    "        time.sleep(2)   # prevents server issues\n",
    "\n",
    "        page_num += 1\n",
    "        \n",
    "        URL = URL_base_1 + str(page_num)\n",
    "    \n",
    "        page = requests.get(URL)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        \n",
    "        # stops function at last page\n",
    "        if len(soup) == 0:\n",
    "            print('DONE: page', page_num, 'does not exist :-(')\n",
    "            break\n",
    "            \n",
    "        # parsing each listing\n",
    "        try:\n",
    "            listings = soup.find_all('div', class_ = \"box tight listing premium\")\n",
    "           \n",
    "            for listing in listings:\n",
    "                entries_completed += 1\n",
    "                internal_dic = {}\n",
    "        \n",
    "                model = listing.find('h3', class_ = \"model\")\n",
    "                internal_dic['model'] = model.text\n",
    "    \n",
    "                try:\n",
    "                    price = listing.find('h4', class_ = \"price\")\n",
    "                    internal_dic['price'] = price.text\n",
    "                except AttributeError:\n",
    "                    print(\"no price found for #\", entries_completed)\n",
    "            \n",
    "                try:\n",
    "                    specs = listing.find('ul', class_ =\"specs\")\n",
    "                    internal_dic['specs'] = specs.text\n",
    "                except AttributeError:\n",
    "                    print(\"no specs found for #\", entries_completed)\n",
    "                \n",
    "                try:\n",
    "                    descrip = listing.find('div', class_ =\"description\")\n",
    "                    internal_dic['descrip'] = descrip.text\n",
    "                except AttributeError:\n",
    "                    print(\"no descrip found for #\", entries_completed)\n",
    "        \n",
    "                listing_dic[entries_completed] = internal_dic\n",
    "        \n",
    "        except AttributeError:\n",
    "            print(\"ERROR: IN #\" ,entries_completed)\n",
    "        \n",
    "        \n",
    "        print(\"COMPLETED: page #\", page_num)\n",
    "        \n",
    "    print(\"COMPLETED: WEB SCRAPE 1\")\n",
    "    return listing_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse miles from messy 'specs' output\n",
    "def parse_miles(listing_dic):\n",
    "    \n",
    "    miles = {}\n",
    "    \n",
    "    for key in listing_dic:\n",
    "        \n",
    "        nested_key = listing_dic[key]\n",
    "        \n",
    "        for item in nested_key:\n",
    "            if item == 'specs':\n",
    "                try:\n",
    "                    specs_split = nested_key[item].split('\\n')\n",
    "                    descrip = listing_dic[key]['descrip']\n",
    "                    if 'miles' in specs_split[-2]:\n",
    "                        miles[key] = specs_split[-2]\n",
    "                    elif 'miles' in descrip:\n",
    "                        descrip_1 = descrip.lower()\n",
    "                        pos = descrip_1.find('miles')\n",
    "                        start = pos-16\n",
    "                        end = pos+5\n",
    "                        miles[key] = descrip_1[start:end]\n",
    "                except TypeError:\n",
    "                    continue\n",
    "\n",
    "    # getting rid of unneeded text  \n",
    "    to_replace = ['original', 'less than', 'actual', 'only', 'documented']\n",
    "    num_list = ['0','1','2','3','4','5','6','7','8','9']\n",
    "    \n",
    "    for key in miles:\n",
    "        \n",
    "        string = miles[key].lower()\n",
    "\n",
    "        #replacing unnecessary words\n",
    "        for word in to_replace:\n",
    "                string = string.replace(word, '')\n",
    "                \n",
    "        string = string.replace('k', '000')\n",
    "        \n",
    "        miles[key] = string\n",
    "        \n",
    "        #extracting only numeric characters from strings\n",
    "        string_cleaned = []\n",
    "        for letter in string:\n",
    "            if letter in num_list:\n",
    "                string_cleaned.append(letter)\n",
    "        \n",
    "        cleaned_miles = ''.join(string_cleaned)\n",
    "        \n",
    "        #flagging suspiciously low miles\n",
    "        if len(cleaned_miles) <= 3:\n",
    "            cleaned_miles = cleaned_miles + '-suspicious'\n",
    "            print(key,':',cleaned_miles)\n",
    "            \n",
    "        #flagging suspiciously high miles\n",
    "        elif len(cleaned_miles) > 6:\n",
    "            cleaned_miles = cleaned_miles + '-suspicious'\n",
    "            print(key,':',cleaned_miles)\n",
    "\n",
    "        miles[key] = cleaned_miles\n",
    "        \n",
    "        listing_dic[key]['mileage'] = miles[key]\n",
    "    \n",
    "    print('COMPLETED: MILES PARSED')\n",
    "    return listing_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_color(listing_dic):\n",
    "    \n",
    "    colors = {}\n",
    "    color_errors = []\n",
    "    num_list = ['0','1','2','3','4','5','6','7','8','9']\n",
    "    \n",
    "    for key in listing_dic:\n",
    "        nested_key = listing_dic[key]\n",
    "        for item in nested_key:\n",
    "            if item == 'specs':\n",
    "                specs_split = nested_key['specs'].split('\\n')\n",
    "                    \n",
    "                #finding listings where engine mislabeled as color\n",
    "                for i in specs_split[3]:\n",
    "                    colors[key] = specs_split[3]\n",
    "                    if i in num_list:\n",
    "                        color_errors.append(key)\n",
    "    for key in colors:\n",
    "        if key in color_errors:\n",
    "            continue\n",
    "        else:\n",
    "            listing_dic[key]['ext_color'] = colors[key]\n",
    "    \n",
    "    print('COMPLETED: COLORS PARSED')\n",
    "    return listing_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_at_df(listing_dic):\n",
    "    df = pd.DataFrame.from_dict(listing_dic)\n",
    "    at = df.transpose()\n",
    "    \n",
    "    print('COMPLETED: DF AT CREATED')\n",
    "    return at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning specific to this web source\n",
    "def basic_cleaning_at(at):\n",
    "    \n",
    "    #Cleaning model col\n",
    "    at['model'] = at['model'].str.replace('New', '')\n",
    "\n",
    "    \n",
    "    #creating year column\n",
    "    at['year'] = at['model'].str.split()\n",
    "    at['year'] = at['year'].apply(lambda x: x[0])\n",
    "\n",
    "    #creating model_name\n",
    "    try:\n",
    "        at['model_name'] = at['model'].str.split()\n",
    "        at['model_name'] = at['model_name'].apply(lambda x: x[1:])\n",
    "        at['model_name'] = at['model_name'].apply(lambda x: ' '.join(x))\n",
    "    except KeyError:\n",
    "        at['model_name'] = at['error!']\n",
    "        print('error with model_name?!')\n",
    "    \n",
    "    #simplifying price\n",
    "    at['price'] = at['price'].str.replace('$','')\n",
    "    at['price'] = at['price'].str.replace(',','')\n",
    "    at['price'] = at['price'].str.strip()\n",
    "    at['price'] = at['price'].replace('For Auction', np.nan)\n",
    "    at['price'] = at['price'].replace('Call for Price', np.nan)\n",
    "    \n",
    "    #creating trim\n",
    "    at['trim'] = at['model_name']\n",
    "\n",
    "    at['trim'] = at['trim'].str.lower()\n",
    "    #at['trim'] = at['trim'].str.replace(make, '')\n",
    "    #at['trim'] = at['trim'].str.replace(model, '')\n",
    "\n",
    "    at['trim'] = at['trim'].str.replace('0', '')\n",
    "    at['trim'] = at['trim'].str.replace('1', '')\n",
    "    at['trim'] = at['trim'].str.replace('2', '')\n",
    "    at['trim'] = at['trim'].str.replace('3', '')\n",
    "    at['trim'] = at['trim'].str.replace('4', '')\n",
    "    at['trim'] = at['trim'].str.replace('5', '')\n",
    "    at['trim'] = at['trim'].str.replace('6', '')\n",
    "    at['trim'] = at['trim'].str.replace('7', '')\n",
    "    at['trim'] = at['trim'].str.replace('8', '')\n",
    "    at['trim'] = at['trim'].str.replace('9', '')\n",
    "\n",
    "    at['trim'] = at['trim'].str.strip()\n",
    "\n",
    "    #clean descrip\n",
    "    at['descrip'] = at['descrip'].str.replace('\\n','')\n",
    "    \n",
    "    #drop unneeded cols\n",
    "    at = at.drop(['model', 'specs'], axis = 1)\n",
    "    \n",
    "    print('COMPLETED: AT BASIC CLEANING')\n",
    "    return at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_scrape_2(URL_base_2):\n",
    "    \n",
    "    listing_dic = {} # dictionary for storing scraped listings\n",
    "    entries_completed = 0 # entry counter\n",
    "    page_num = 0 # listings page counter\n",
    "    \n",
    "\n",
    "    while page_num <= 100:\n",
    "        page_num += 1\n",
    "        \n",
    "        URL = URL_base_2 + str(page_num) + '&searchRadius=5000&sort[]=best_match'\n",
    "    \n",
    "        page = requests.get(URL)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        main = soup.find(class_ = \"row row-2 margin-bottom-3\")\n",
    "        \n",
    "        #finding last page\n",
    "        not_found = soup.find('span', class_=\"d-none d-md-block\")\n",
    "        if not not_found is None:\n",
    "            print('DONE: page', page_num, 'does not exist :-(')\n",
    "            break\n",
    "        else:\n",
    "            try:\n",
    "                listings = main.find_all('div', class_ = \"card-content vehicle-card-body\")\n",
    "            \n",
    "                for listing in listings:\n",
    "                    entries_completed += 1\n",
    "                    internal_dic = {}\n",
    "    \n",
    "                    year = listing.find('span', class_ = \"vehicle-card-year font-size-1\")\n",
    "                    internal_dic['model_year'] = year.text\n",
    "    \n",
    "                    make = listing.find('span', class_ =\"vehicle-header-make-model text-truncate\")\n",
    "                    internal_dic['make_model'] = make.text\n",
    "    \n",
    "                    try:\n",
    "                        trim = listing.find('div', class_ = \"font-size-1 text-truncate\")\n",
    "                        internal_dic['trim'] = trim.text\n",
    "                    except AttributeError:\n",
    "                        print(\"no trim found for #\", entries_completed)\n",
    "    \n",
    "                    try:\n",
    "                        price_label = listing.find('span', class_ = \"graph-icon-title margin-left-1 vehicle-card-price-rating-label text-truncate font-weight-bold\")\n",
    "                        internal_dic['price_label'] = price_label.text\n",
    "                    except AttributeError:\n",
    "                        print(\"no price_label found for #\", entries_completed)\n",
    "    \n",
    "                    try:\n",
    "                        price = listing.find('h4', class_ = 'heading-3 margin-y-1 font-weight-bold')\n",
    "                        internal_dic['price'] = price.text\n",
    "                    except AttributeError:\n",
    "                        print(\"no price found for #\", entries_completed)\n",
    "    \n",
    "                    try:\n",
    "                        mileage = listing.find('div', class_ = 'd-flex w-100 justify-content-between')\n",
    "                        internal_dic['mileage'] = mileage.text\n",
    "                    except AttributeError:\n",
    "                        print(\"no mileage found for #\", entries_completed)\n",
    "\n",
    "                    try:\n",
    "                        location = listing.find('div', class_ = 'vehicle-card-location font-size-1 margin-top-1')\n",
    "                        internal_dic['location'] = location.text\n",
    "                    except AttributeError:\n",
    "                        print(\"no location found for #\", entries_completed)\n",
    "        \n",
    "                    try:\n",
    "                        color = listing.find('div', class_ = 'vehicle-card-location font-size-1 margin-top-1 text-truncate')\n",
    "                        internal_dic['color'] = color.text\n",
    "                    except AttributeError:\n",
    "                        print(\"no color found for #\", entries_completed)\n",
    "    \n",
    "                    listing_dic[entries_completed] = internal_dic\n",
    "            except AttributeError:\n",
    "                print(\"ERROR: IN #\" ,entries_completed)\n",
    "                \n",
    "        print(\"COMPLETED: page #\", page_num)\n",
    "        time.sleep(2) #prevents server issues\n",
    "    \n",
    "    \n",
    "    print(\"COMPLETED: WEB SCRAPE 2\")\n",
    "    return listing_dic   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tc_df(listing_dic_tc):\n",
    "    tc = pd.DataFrame.from_dict(listing_dic_tc)\n",
    "    tc = tc.transpose()\n",
    "    \n",
    "    print('COMPLETED: DF TC CREATED')\n",
    "    return tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_cleaning_tc(tc):\n",
    "    \n",
    "    #cleaning price\n",
    "    tc['price'] = tc['price'].str.replace('$', '')\n",
    "    tc['price'] = tc['price'].str.replace(',','')\n",
    "    tc['price'] = tc['price'].str.strip()\n",
    "\n",
    "\n",
    "    #cleaning mileage\n",
    "    tc['mileage'] = tc['mileage'].str.replace('Discount Available', '')\n",
    "    tc['mileage'] = tc['mileage'].str.replace('Upfront Price Available', '')\n",
    "    tc['mileage'] = tc['mileage'].str.replace('miles', '')\n",
    "    tc['mileage'] = tc['mileage'].str.replace(',','')\n",
    "    tc['mileage'] = tc['mileage'].str.strip()\n",
    "    \n",
    "    #cleaning and seperating location\n",
    "\n",
    "    tc['location'] = tc['location'].str.split('-')\n",
    "    tc['city'] = tc['location'].apply(lambda x: x[1])\n",
    "    tc['city'] = tc['city'].str.split(',')\n",
    "    tc['state'] = tc['city']\n",
    "    tc['city'] = tc['city'].apply(lambda x: x[0])\n",
    "    tc['state'] = tc['state'].apply(lambda x: x[1:])\n",
    "    tc['state'] = tc['state'].apply(lambda x: ''.join(x))\n",
    "    \n",
    "    ### making ext_color and int_color ###\n",
    "    tc['color'] = tc['color'].str.split(',')\n",
    "    \n",
    "    tc['ext_color'] = tc['color'].apply(lambda x: x[0])\n",
    "    tc['ext_color'] = tc['ext_color'].str.replace('exterior','')\n",
    "\n",
    "    tc['int_color'] = tc['color'].apply(lambda x: x[1])\n",
    "    tc['int_color'] = tc['int_color'].str.replace('interior','')\n",
    "    \n",
    "    #drop redudant cols\n",
    "    tc = tc.drop(['location', 'color'], axis = 1)\n",
    "    \n",
    "    print(\"COMPLETED: TC BASIC CLEANING\")\n",
    "    return tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run all functions\n",
    "def scrape(URL_base_1, URL_base_2, save_path_1, save_path_2):\n",
    "    \n",
    "    #running at\n",
    "    listing_dic_at = web_scrape_1(URL_base_1)\n",
    "    listing_dic_at = parse_miles(listing_dic_at)\n",
    "    listing_dic_at = parse_color(listing_dic_at)\n",
    "    at = create_at_df(listing_dic_at)\n",
    "    at = basic_cleaning_at(at)\n",
    "    \n",
    "    #running tc\n",
    "    listing_dic_tc = web_scrape_2(URL_base_2)\n",
    "    tc = create_tc_df(listing_dic_tc)\n",
    "    \n",
    "    #save as csv\n",
    "    at.to_csv(save_path_1)\n",
    "    tc.to_csv(save_path_2)\n",
    "    \n",
    "    tc = basic_cleaning_tc(tc)\n",
    "    tc.to_csv(save_path_2)\n",
    "    \n",
    "    print(\"COMPLETED: AT AND TC SAVED AS CSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = pd.read_csv('/Users/avacheevers/Documents/Cars/both.csv')\n",
    "model_dic = models.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in model_dic['Car']:\n",
    "    scrape(\n",
    "    model_dic['AT URL'][key],\n",
    "    model_dic['TC URL'][key],\n",
    "    '/Users/avacheevers/Documents/Cars/1_'+model_dic['Car'][key]+'_at.csv',\n",
    "    '/Users/avacheevers/Documents/Cars/1_'+model_dic['Car'][key]+'_tc.csv'\n",
    "    )\n",
    "    print(key, \"completed part 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
